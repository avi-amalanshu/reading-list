# Inference and Learning
I think people just getting into ML often try to get away with the bare minimum of probability. 
I am a staunch believer in studying in-context from statistics and information theory, even if your personal interests
don't require them (if they do, I suggest a more rigorous treatment than what I prescribe here). Besides gaining a more 
fundamental and usable understanding, the bigger picture-view makes it easier to focus on the right things. And, extra insight never hurt anyone.
## Probability & Statistics Basics
1. Though quite brief, worth a skim for context: Pishro-Nik's online probability text. (H. Pishro-Nik, "Introduction to probability, statistics, and random processes", available at [probabilitycourse.com](https://www.probabilitycourse.com), Kappa Research LLC, 2014).
2. Bertsekas and Tsitsiklis' book is not only a great introduction to the tools you'll need but is also covered through [free lecture notes](https://www-sop.inria.fr/members/Giovanni.Neglia/probas/bertsekas_tsitsiklis_probability.pdf) and a great [MIT OCW course w/ lecture videos](https://ocw.mit.edu/courses/res-6-012-introduction-to-probability-spring-2018/). (D.P. Bertsekas, J.N. Tsitsiklis "Introduction to probability" 2nd ed, Athena Scientific, 2008).
3. The first few chapters of Steven Kay's parametric estimation book will give you an idea of theoretical constraints such as bias-variance tradeoff. Plus, it's a new (sigproc) angle to look at the same thing; always useful. If you find yourself interested in Bayesian methods the second half of the book may also be useful. (S.M. Kay "Fundamentals of statistical signal processing: estimation theory", Prentice-Hall signal processing series, 1993)
## Information Theory and Inference
1. [David MacKay's book](https://www.inference.org.uk/itprnn/book.pdf) is a fantastic and very readable introduction to the math behind inference. The book mentions which chapters are useful for which readers. Could skip the stuff on coding theory but it's fun + useful for some emerging ML topics. (D.J. MacKay "Information Theory,
Inference, and Learning Algorithms" v7.2, Cambridge University Press, 2005)
2. If you're interested in insight on issues with sample complexity/data efficiency, Chapter 11 of [Cover & Thomas](https://cs-114.org/wp-content/uploads/2015/01/Elements_of_Information_Theory_Elements.pdf) might be useful. (T.M. Cover, J.A. Thomas "Elements of Information Theory" 2nd ed, Wiley, 1991)

## Alternatively...
I've heard [ISLR/ISLP](https://www.statlearning.com) is great and covers pretty much all the important parts of what I've described above. I haven't read it myself (it's rather new), but it looks great if you're into less math rigor and more English-heavy explanations. The authors are folks who have developed some pretty fundamental works in statistics in their own research. (G. James, D. Witten, T. Hastie, R. Tibshirani, J. Taylor "An introduction to statistical learning" w/ Applications in Python, Springer, 2023). 

## State-of-the-Art Learning
1. Though some think it's long in the tooth, at least the first two sections of the OG [Deep Learning book](https://www.deeplearningbook.org) are excellent background. (I. Goodfellow, Y. Bengio, A. Courville "Deep learning", MIT Press, 2016).
2. [The Bishops' Deep Learning book](https://www.bishopbook.com) is a more modern (as of '24) book. I haven't read this myself either but the author's OG ML book, Pattern Recognition & Machine Intelligence, was great. (C.M. Bishop, H. Bishop "Deep Learning Foundations and Concepts", Springer, 2024).
## Special Topics
WIP